{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/muhammadkararmirza/cpu-vs-gpu?scriptVersionId=180014094\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-27T08:19:07.916825Z","iopub.execute_input":"2024-05-27T08:19:07.917507Z","iopub.status.idle":"2024-05-27T08:19:07.925414Z","shell.execute_reply.started":"2024-05-27T08:19:07.917469Z","shell.execute_reply":"2024-05-27T08:19:07.924142Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n    \nprint(\"You are using\", device, \"device\")","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:20:21.745518Z","iopub.execute_input":"2024-05-27T08:20:21.745846Z","iopub.status.idle":"2024-05-27T08:20:25.365307Z","shell.execute_reply.started":"2024-05-27T08:20:21.745817Z","shell.execute_reply":"2024-05-27T08:20:25.364142Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"You are using cuda device\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n\n    num_devices = torch.cuda.device_count()\n    print(f\"Number of CUDA devices: {num_devices}\")\n\n\n    for i in range(num_devices):\n        device = torch.cuda.get_device_properties(i)\n        print(f\"CUDA Device {i} - Name: {device.name}, Memory: {device.total_memory / 1024**3:.2f}GB\")\nelse:\n    print(\"CUDA is not available.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:24:17.370071Z","iopub.execute_input":"2024-05-27T08:24:17.370963Z","iopub.status.idle":"2024-05-27T08:24:17.377165Z","shell.execute_reply.started":"2024-05-27T08:24:17.370928Z","shell.execute_reply":"2024-05-27T08:24:17.376023Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of CUDA devices: 2\nCUDA Device 0 - Name: Tesla T4, Memory: 14.75GB\nCUDA Device 1 - Name: Tesla T4, Memory: 14.75GB\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\n\nmatrix_size = 42*256\n\nx = torch.randn(matrix_size, matrix_size)\ny = torch.randn(matrix_size, matrix_size)\n\nprint(\"************ CPU SPEED ***************\")\nstart = time.time()\nresult = torch.matmul(x,y)\nprint(time.time() - start)\nprint(\"Device Verfication:\", result.device)\n\nx_gpu = x.to(device)\ny_gpu = y.to(device)\ntorch.cuda.synchronize()\n\nfor i in range(3):\n    print(\"************ GPU SPEED ***************\")\n    start = time.time()\n    result_gpu = torch.matmul(x_gpu,y_gpu)\n    torch.cuda.synchronize()\n    print(time.time() - start)\n    print(\"Device Verification:\", result_gpu.device)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T08:21:46.800033Z","iopub.execute_input":"2024-05-27T08:21:46.801005Z","iopub.status.idle":"2024-05-27T08:21:59.755208Z","shell.execute_reply.started":"2024-05-27T08:21:46.80097Z","shell.execute_reply":"2024-05-27T08:21:59.754397Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"************ CPU SPEED ***************\n8.538119316101074\nDevice Verfication: cpu\n************ GPU SPEED ***************\n0.6668496131896973\nDevice Verification: cuda:0\n************ GPU SPEED ***************\n0.5416319370269775\nDevice Verification: cuda:0\n************ GPU SPEED ***************\n0.523665189743042\nDevice Verification: cuda:0\n","output_type":"stream"}]}]}